{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20d509ee",
   "metadata": {},
   "source": [
    "# Lets Run LLM from hugging face on the laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4105e568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a65a7450",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_API_KEY = os.environ.get(\"HUGGING_FACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e95cee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"lmsys/fastchat-t5-3b-v1.0\"  #name of the model, you can select whichever better for your application this is most suitable for text generation. you can find this info right below the name of the model on HF.\n",
    "filenames = [\n",
    "        \"pytorch_model.bin\", \"added_tokens.json\", \"config.json\", \"generation_config.json\", \n",
    "        \"special_tokens_map.json\", \"spiece.model\", \"tokenizer_config.json\"\n",
    "]  ## Files and version tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "136b8711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\*****\\.cache\\huggingface\\hub\\models--lmsys--fastchat-t5-3b-v1.0\\snapshots\\0b1da230a891854102d749b93f7ddf1f18a81024\\pytorch_model.bin\n",
      "C:\\Users\\*****\\.cache\\huggingface\\hub\\models--lmsys--fastchat-t5-3b-v1.0\\snapshots\\0b1da230a891854102d749b93f7ddf1f18a81024\\added_tokens.json\n",
      "C:\\Users\\*****\\.cache\\huggingface\\hub\\models--lmsys--fastchat-t5-3b-v1.0\\snapshots\\0b1da230a891854102d749b93f7ddf1f18a81024\\config.json\n",
      "C:\\Users\\*****\\.cache\\huggingface\\hub\\models--lmsys--fastchat-t5-3b-v1.0\\snapshots\\0b1da230a891854102d749b93f7ddf1f18a81024\\generation_config.json\n",
      "C:\\Users\\*****\\.cache\\huggingface\\hub\\models--lmsys--fastchat-t5-3b-v1.0\\snapshots\\0b1da230a891854102d749b93f7ddf1f18a81024\\special_tokens_map.json\n",
      "C:\\Users\\*****\\.cache\\huggingface\\hub\\models--lmsys--fastchat-t5-3b-v1.0\\snapshots\\0b1da230a891854102d749b93f7ddf1f18a81024\\spiece.model\n",
      "C:\\Users\\*****\\.cache\\huggingface\\hub\\models--lmsys--fastchat-t5-3b-v1.0\\snapshots\\0b1da230a891854102d749b93f7ddf1f18a81024\\tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "for filename in filenames:\n",
    "        downloaded_model_path = hf_hub_download(\n",
    "                    repo_id=model_id,\n",
    "                    filename=filename,\n",
    "                    token=HUGGING_FACE_API_KEY\n",
    "        )\n",
    "        print(downloaded_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8faf90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\*****\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "pipeline = pipeline(\"text2text-generation\", model=model, device=-1, tokenizer=tokenizer, max_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9164f1be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'iowa'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(\"Name a country which starts with letter 'I'?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc0135fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Some weights of T5ForQuestionAnswering were not initialized from the model checkpoint at lmsys/fastchat-t5-3b-v1.0 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.00023218839487526566, 'start': 262, 'end': 270, 'answer': 'rse of B'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "\n",
    "# Load the question-answering model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_id)\n",
    "\n",
    "# Set up the question-answering pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=-1)\n",
    "\n",
    "# Example usage\n",
    "context = \"The Ganga originates as Bhagirathi from the Gangotri glaciers in the Himalayas at an elevation of about 7010m in Uttarkashi district of Uttarakhand and flows for a total length of about 2525 km up to its outfall into the Bay of Bengal through the former main course of Bhagirathi-Hooghly.\"\n",
    "question = \"Where does Ganga River start and end?\"\n",
    "\n",
    "# Get the answer\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05789040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and.\n"
     ]
    }
   ],
   "source": [
    "context = \"The Ganga originates in Uttarakhand.\"\n",
    "question = \"Where does Ganga River originate?\"\n",
    "\n",
    "# Get the answer\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "\n",
    "# Check if an answer is found\n",
    "if result['answer']:\n",
    "    print(result['answer'])\n",
    "else:\n",
    "    print(\"No answer found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8029e8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#poor Model response, lets try another model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "311fa263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\*****\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece0986c76924a9da1e91ca3677d8855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of France?\n",
      "\n",
      "# Answer\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Text generation\n",
    "input_text = \"What is the capital of France?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "\n",
    "# Decode and print result\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "465db527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of India?\n",
      "\n",
      "# Answer\n",
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "# Text generation\n",
    "input_text = \"What is the capital of India?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "\n",
    "# Decode and print result\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b42487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a46067",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
